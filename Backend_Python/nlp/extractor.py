import re
import os
import logging
import pandas as pd
import torch
import nltk
from underthesea import word_tokenize, text_normalize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from nlp.comparator import normalize_text
from models.keywords import KEYWORD_CATEGORIES

# T·∫£i t√†i nguy√™n NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Danh s√°ch stop words ti·∫øng Vi·ªát
STOP_WORDS = {
    "v√†", "c·ªßa", "cho", "l√†", "ƒë·ªÉ", "trong", "v·ªõi", "c√≥", "ƒë∆∞·ª£c", "kh√¥ng", "t·∫°i", "t·ª´", "tr√™n",
    "d∆∞·ªõi", "gi·ªØa", "khi", "n·∫øu", "ho·∫∑c", "nh∆∞ng", "th√¨", "m√†", "b·ªüi", "v√¨", "n√™n", "do",
    "c≈©ng", "ƒë√£", "ƒëang", "s·∫Ω", "v·∫´n", "ch∆∞a", "l·∫°i", "m·ªõi", "v·ª´a", "ngay", "lu√¥n", "th∆∞·ªùng",
    "hay", "ƒë·ªÅu", "r·∫•t", "qu√°", "l·∫Øm", "h·∫øt", "c·∫£", "m·ªçi", "m·ªói", "nhi·ªÅu", "√≠t", "h∆°n", "k√©m"
}

def preprocess_text(text):
    """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát v√† t√°ch d√≤ng ch√≠nh x√°c h∆°n."""
    try:
        # T√°ch vƒÉn b·∫£n th√†nh c√°c d√≤ng c∆° b·∫£n
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        merged_lines = []
        buffer = ""

        # Danh s√°ch c√°c ti√™u ƒë·ªÅ c·∫ßn t√°ch ri√™ng
        section_headers = {
            "m·ª•c ti√™u ngh·ªÅ nghi·ªáp", "kinh nghi·ªám l√†m vi·ªác", "th√¥ng tin c√° nh√¢n",
            "h·ªçc v·∫•n", "k·ªπ nƒÉng", "danh hi·ªáu v√† gi·∫£i th∆∞·ªüng", "ch·ª©ng ch·ªâ",
            "ho·∫°t ƒë·ªông", "d·ª± √°n"
        }

        # Regex cho c√°c tr∆∞·ªùng ƒë·∫∑c bi·ªát
        email_regex = r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+"
        phone_regex = r"(0|\+84)\s*[0-9]{9,10}|\(\d{3,4}\)\s*[0-9]{7,8}"
        name_regex = r"^(?:[A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∏·ª∂·ª¥][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]*\s){1,3}[A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∏·ª∂·ª¥][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]*$"
        birth_date_line_regex = r"^ngay\s*sinh\s*:"
        address_line_regex = r"^dia\s*chi\s*:"
        gender_line_regex = r"^gioi\s*tinh\s*:"

        for line in lines:
            normalized_line = text_normalize(line.lower())
            tokens = word_tokenize(normalized_line)
            tokens = [t for t in tokens if t not in STOP_WORDS]

            # Lo·∫°i b·ªè d√≤ng ch·ªâ c√≥ 1 ch·ªØ kh√¥ng mang ng·ªØ nghƒ©a
            if len(tokens) <= 1 and (tokens[0] in STOP_WORDS or re.match(r"[\W]+", tokens[0])):
                logger.debug(f"B·ªè qua d√≤ng kh√¥ng mang ng·ªØ nghƒ©a: {tokens}")
                continue

            # T√°ch s·ªë ƒëi·ªán tho·∫°i, email ho·∫∑c t√™n ri√™ng n·∫øu xu·∫•t hi·ªán trong d√≤ng
            phone_match = re.search(phone_regex, line, re.IGNORECASE)
            email_match = re.search(email_regex, line, re.IGNORECASE)
            name_match = re.match(name_regex, line, re.IGNORECASE)

            # T√°ch ti√™u ƒë·ªÅ n·∫øu d√≤ng ch·ª©a ti√™u ƒë·ªÅ
            found_header = None
            for header in section_headers:
                if header in normalized_line:
                    found_header = header
                    break

            if found_header:
                # T√°ch ti√™u ƒë·ªÅ v√† ph·∫ßn c√≤n l·∫°i
                header_index = normalized_line.find(found_header)
                before_header = line[:header_index].strip()
                after_header = line[header_index + len(found_header):].strip()

                if before_header:
                    if re.match(name_regex, before_header, re.IGNORECASE):
                        merged_lines.append(before_header)
                    elif re.match(phone_regex, before_header, re.IGNORECASE):
                        merged_lines.append(before_header)
                    elif re.match(email_regex, before_header, re.IGNORECASE):
                        merged_lines.append(before_header)
                    else:
                        buffer += " " + before_header if buffer else before_header
                merged_lines.append(found_header.title())
                if after_header:
                    buffer = after_header
                continue

            # üö´ B·ªè watermark ho·∫∑c d√≤ng th·ª´a
            if "topcv.vn" in normalized_line:
                continue

            # Gi·ªØ nguy√™n c√°c d√≤ng th√¥ng tin c√° nh√¢n quan tr·ªçng
            if (re.match(birth_date_line_regex, normalized_line, re.IGNORECASE) or
                    re.match(gender_line_regex, normalized_line, re.IGNORECASE) or
                    re.match(address_line_regex, normalized_line, re.IGNORECASE)):
                merged_lines.append(line)
                continue

            # T√°ch s·ªë ƒëi·ªán tho·∫°i n·∫øu c√≥
            if phone_match:
                phone_number = phone_match.group(0)
                merged_lines.append(phone_number)
                remaining_text = line.replace(phone_number, "").strip()
                if remaining_text:
                    buffer = remaining_text
                continue

            # T√°ch email n·∫øu c√≥
            if email_match:
                email = email_match.group(0)
                merged_lines.append(email)
                remaining_text = line.replace(email, "").strip()
                if remaining_text:
                    buffer = remaining_text
                continue

            # T√°ch t√™n n·∫øu c√≥
            if name_match and len(line.split()) <= 4:
                merged_lines.append(line)
                continue

            # G·ªôp d√≤ng ng·∫Øn ho·∫∑c kh√¥ng mang ng·ªØ nghƒ©a v√†o buffer
            if buffer and len(buffer.split()) < 4 and not any(keyword in normalized_line for keyword in
                                                              KEYWORD_CATEGORIES["career_objective"] + KEYWORD_CATEGORIES[
                                                                  "experience"] + KEYWORD_CATEGORIES["skills_technology"] +
                                                              KEYWORD_CATEGORIES["skills_soft"] + KEYWORD_CATEGORIES[
                                                                  "awards"]):
                buffer += " " + line
            else:
                if buffer:
                    merged_lines.append(buffer)
                buffer = line

        # Th√™m d√≤ng cu·ªëi c√πng trong buffer n·∫øu c√≥
        if buffer:
            merged_lines.append(buffer)

        # Lo·∫°i b·ªè c√°c d√≤ng ch·ªâ c√≥ d·∫•u c√¢u ho·∫∑c kh√¥ng mang ng·ªØ nghƒ©a
        final_lines = []
        for line in merged_lines:
            tokens = word_tokenize(line)
            if len(tokens) <= 1 and re.match(r"[\W]+", tokens[0]):
                logger.debug(f"B·ªè qua d√≤ng kh√¥ng mang ng·ªØ nghƒ©a: {line}")
                continue
            final_lines.append(line)

        logger.info(f"K·∫øt qu·∫£ ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (NLP): {len(final_lines)} d√≤ng")  # Log k·∫øt qu·∫£ NLP
        return final_lines
    except Exception as e:
        logger.error(f"L·ªói trong preprocess_text: {str(e)}")
        return []

class CVDataset(Dataset):
    """Dataset t√πy ch·ªânh cho d·ªØ li·ªáu CV."""
    def __init__(self, texts, labels, tokenizer, max_length=64):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        tokenized_text = word_tokenize(text)
        encoding = self.tokenizer(
            " ".join(tokenized_text),
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def train_phobert_model(df, model_path, tokenizer_path, max_length=128, epochs=20):
    """Hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT v·ªõi tham s·ªë t·ªëi ∆∞u v√† l∆∞u checkpoint t·ªët nh·∫•t."""
    try:
        logger.info("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT")

        # Chu·∫©n b·ªã nh√£n
        label_set = sorted(list(set(df['label'])))  # s·∫Øp x·∫øp ƒë·ªÉ ·ªïn ƒë·ªãnh mapping
        label_to_id = {label: idx for idx, label in enumerate(label_set)}

        texts = df['text'].tolist()
        labels = [label_to_id[label] for label in df['label']]

        # Chia train/validation
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )

        # Tokenizer & model PhoBERT
        tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
        model = AutoModelForSequenceClassification.from_pretrained(
            "vinai/phobert-base",
            num_labels=len(label_set)
        )

        # Dataset
        train_dataset = CVDataset(train_texts, train_labels, tokenizer, max_length)
        val_dataset = CVDataset(val_texts, val_labels, tokenizer, max_length)

        from transformers import TrainingArguments, Trainer

        # T√≠nh warmup_steps (10% t·ªïng steps)
        total_steps = (len(train_dataset) // 8) * epochs   # batch_size=8
        warmup_steps = int(total_steps * 0.1)

        # Metric ƒë·ªÉ ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t
        from sklearn.metrics import accuracy_score, f1_score

        def compute_metrics(eval_pred):
            logits, labels = eval_pred
            preds = logits.argmax(-1)
            acc = accuracy_score(labels, preds)
            f1 = f1_score(labels, preds, average="macro")
            return {"accuracy": acc, "f1": f1}

        # Training arguments t·ªëi ∆∞u
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs=epochs,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=16,
            learning_rate=2e-5,
            warmup_steps=warmup_steps,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=100,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            save_total_limit=3,                      # ch·ªâ gi·ªØ 3 checkpoint g·∫ßn nh·∫•t
            load_best_model_at_end=True,             # t·ª± ƒë·ªông l·∫•y m√¥ h√¨nh t·ªët nh·∫•t
            metric_for_best_model="eval_loss",       # c√≥ th·ªÉ ƒë·ªïi th√†nh "eval_f1"
            greater_is_better=False,                 # v√¨ eval_loss c√†ng th·∫•p c√†ng t·ªët
            gradient_accumulation_steps=2,           # batch hi·ªáu d·ª•ng l·ªõn h∆°n
            max_grad_norm=1.0,                       # gradient clipping
            fp16=False                               # True n·∫øu GPU h·ªó tr·ª£
        )

        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics
        )

        # Hu·∫•n luy·ªán
        trainer.train()

        # ƒê√°nh gi√° m√¥ h√¨nh t·ªët nh·∫•t
        eval_results = trainer.evaluate()
        logger.info(f"K·∫øt qu·∫£ ƒë√°nh gi√° PhoBERT: {eval_results}")

        # L∆∞u model & tokenizer (best checkpoint)
        os.makedirs(model_path, exist_ok=True)
        trainer.model.save_pretrained(model_path)
        tokenizer.save_pretrained(tokenizer_path)
        logger.info("L∆∞u m√¥ h√¨nh v√† tokenizer PhoBERT th√†nh c√¥ng")

        # L∆∞u file nh√£n
        with open(os.path.join(model_path, 'labels.txt'), 'w', encoding='utf-8') as f:
            for label in label_set:
                f.write(label + '\n')

        return trainer.model, tokenizer, label_set

    except Exception as e:
        logger.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT: {str(e)}")
        raise

def extract_fields_from_text(text):
    """Tr√≠ch xu·∫•t c√°c tr∆∞·ªùng th√¥ng tin t·ª´ vƒÉn b·∫£n CV."""
    try:
        logger.info("B·∫Øt ƒë·∫ßu qu√° tr√¨nh tr√≠ch xu·∫•t CV")

        base_dir = os.path.dirname(os.path.abspath(__file__))
        training_data_path = os.path.join(base_dir, '..', 'models', 'training_data.csv')
        logger.info(f"Th∆∞ m·ª•c hi·ªán t·∫°i: {os.getcwd()}")
        logger.info(f"ƒêang c·ªë g·∫Øng truy c·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán t·∫°i: {os.path.abspath(training_data_path)}")

        if not os.path.exists(training_data_path):
            logger.error(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu hu·∫•n luy·ªán: {training_data_path}")
            return {
                "status": "failed",
                "error_message": f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu hu·∫•n luy·ªán: {training_data_path}"
            }

        model_path = os.path.join(base_dir, 'phobert_cv_classifier')
        tokenizer_path = os.path.join(base_dir, 'phobert_cv_classifier')
        if os.path.exists(model_path) and os.path.exists(tokenizer_path):
            try:
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
                model = AutoModelForSequenceClassification.from_pretrained(model_path)
                with open(os.path.join(model_path, 'labels.txt'), 'r') as f:
                    label_set = [line.strip() for line in f.readlines()]
                logger.info("T·∫£i m√¥ h√¨nh v√† tokenizer PhoBERT ƒë√£ hu·∫•n luy·ªán th√†nh c√¥ng")
            except Exception as e:
                logger.error(f"L·ªói khi t·∫£i m√¥ h√¨nh PhoBERT: {str(e)}")
                return {
                    "status": "failed",
                    "error_message": f"L·ªói khi t·∫£i m√¥ h√¨nh PhoBERT: {str(e)}"
                }
        else:
            logger.info("Hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT m·ªõi")
            try:
                df = pd.read_csv(training_data_path)
                model, tokenizer, label_set = train_phobert_model(df, model_path, tokenizer_path)
            except Exception as e:
                logger.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT: {str(e)}")
                return {
                    "status": "failed",
                    "error_message": f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh PhoBERT: {str(e)}"
                }

        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ƒë·ªÉ g·ªôp c√°c d√≤ng b·ªã ng·∫Øt
        lines = preprocess_text(text)
        if not lines:
            logger.error("Kh√¥ng th·ªÉ ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n CV")
            return {
                "status": "failed",
                "error_message": "Kh√¥ng th·ªÉ ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n CV"
            }
        logger.info(f"ƒêang x·ª≠ l√Ω {len(lines)} d√≤ng")

        predictions = []
        for line in lines:
            try:
                tokenized_text = word_tokenize(line)
                encoding = tokenizer(
                    " ".join(tokenized_text),
                    padding='max_length',
                    truncation=True,
                    max_length=64,
                    return_tensors='pt'
                )
                with torch.no_grad():
                    outputs = model(**encoding)
                    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()
                    pred_label = label_set[predicted_label_id]
                    predictions.append(pred_label)
                    logger.debug(f"K·∫øt qu·∫£ ph√¢n lo·∫°i d√≤ng (ML): '{line[:50]}...' -> {pred_label}")  # Log k·∫øt qu·∫£ ph√¢n lo·∫°i ML
            except Exception as e:
                logger.error(f"L·ªói khi ph√¢n lo·∫°i d√≤ng '{line}': {str(e)}")
                predictions.append("other")  # G√°n nh√£n m·∫∑c ƒë·ªãnh n·∫øu l·ªói
        logger.info(f"Ho√†n th√†nh ph√¢n lo·∫°i cho {len(predictions)} d√≤ng")

        personal_info = {
            "name": [], "email": [], "phone": [], "address": [], "birth_date": [], "gender": [], "other": []
        }
        education, skills, projects, experience, hobbies, career_objective, certificates, awards, activities, references = [], [], [], [], [], [], [], [], [], []

        birth_date_line_regex = r"^ngay\s*sinh\s*:"
        address_line_regex = r"^dia\s*chi\s*:"
        gender_line_regex = r"^gioi\s*tinh\s*:"
        email_regex = r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+"
        phone_regex = r"(0|\+84)\s*[0-9]{9,10}|\(\d{3,4}\)\s*[0-9]{7,8}"
        name_regex = r"^(?:[A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∏·ª∂·ª¥][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]*\s){1,3}[A-Z√Ä√Å√É·∫†·∫¢ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√çƒ®·ªà·ªä√í√ì√ï·ªå·ªé√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö≈®·ª§·ª¶∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∏·ª∂·ª¥][a-z√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]*$"
        address_regex = r"(?:h√† n·ªôi|s√†i g√≤n|ƒë√† n·∫µng|c·∫ßn th∆°|ƒë·ªëng ƒëa|an giang|qu·∫≠n \d+|tp\.?|th√†nh ph·ªë|ph∆∞·ªùng|ktx|xu√¢n kh√°nh|ninh ki·ªÅu|c·∫ßu gi·∫•y|hai b√† tr∆∞ng|ba ƒë√¨nh)[\w\s,]*"
        birth_date_regex = r"\d{2}/\d{2}/\d{4}"
        gender_regex = r"Nam|N·ªØ"
        duration_regex = r"(\d{4}\s*-\s*\d{4}|\d{4}\s*-\s*(?:nay|hi·ªán t·∫°i)|\d{2}/\d{4}\s*-\s*\d{2}/\d{4}|\d{2}/\d{4}\s*-\s*(?:nay|hi·ªán t·∫°i))"
        company_regex = r"(?:c√¥ng ty|company)\s*(?:tnhh|cp|c·ªï ph·∫ßn)\s*[\w\s]+"
        year_regex = r"\d{4}"

        # Theo d√µi ng·ªØ c·∫£nh v√† g·ªôp references
        current_section = None
        reference_buffer = []
        previous_line = ""

        for i, (line, pred) in enumerate(zip(lines, predictions)):
            line_lower = line.lower()
            normalized_line = normalize_text(line)
            logger.debug(f"X·ª≠ l√Ω d√≤ng: {line}, D·ª± ƒëo√°n: {pred}")

            if re.match(birth_date_line_regex, normalized_line, re.IGNORECASE):
                date_match = re.search(r":\s*(\d{1,2}/\d{1,2}/\d{4})", line)
                if date_match:
                    personal_info["birth_date"].append(date_match.group(1))
                    logger.debug(f"Kh·ªõp ng√†y sinh: {line}")
                continue

            if re.match(address_line_regex, normalized_line, re.IGNORECASE):
                address_value = line.split(":", 1)[-1].strip()
                if address_value:
                    personal_info["address"].append(address_value)
                    logger.debug(f"Kh·ªõp ƒë·ªãa ch·ªâ: {line}")
                continue

            if re.match(gender_line_regex, normalized_line, re.IGNORECASE):
                gender_value = line.split(":", 1)[-1].strip()
                if gender_value:
                    personal_info["gender"].append(gender_value)
                    logger.debug(f"Kh·ªõp gi·ªõi t√≠nh: {line}")
                continue

            # Danh s√°ch c√°c ti√™u ƒë·ªÅ v√† √°nh x·∫° v·ªõi danh m·ª•c
            section_headers = {
                "m·ª•c ti√™u ngh·ªÅ nghi·ªáp": "career_objective",
                "h·ªçc v·∫•n": "education",
                "kinh nghi·ªám l√†m vi·ªác": "experience",
                "k·ªπ nƒÉng": "skills",
                "danh hi·ªáu v√† gi·∫£i th∆∞·ªüng": "awards",
                "ch·ª©ng ch·ªâ": "certificates",
                "ho·∫°t ƒë·ªông": "activities",
                "d·ª± √°n": "projects"
            }
            if normalized_line in section_headers:
                current_section = section_headers[normalized_line]
                logger.debug(f"Ph√°t hi·ªán danh m·ª•c: {current_section}")
                continue

            # ∆Øu ti√™n ki·ªÉm tra t·ª´ kh√≥a t·ª´ KEYWORD_CATEGORIES
            keyword_match = None
            for category, keywords in KEYWORD_CATEGORIES.items():
                if isinstance(keywords, list):
                    if any(keyword in normalized_line for keyword in keywords):
                        keyword_match = category
                        break
                elif isinstance(keywords, dict):  # X·ª≠ l√Ω skills_industry
                    for sub_category, sub_keywords in keywords.items():
                        if any(keyword in normalized_line for keyword in sub_keywords):
                            keyword_match = category
                            break
                    if keyword_match:
                        break

            final_pred = keyword_match if keyword_match else (current_section if current_section else pred)
            logger.debug(
                f"D√≤ng: {line}, D·ª± ƒëo√°n: {pred}, Kh·ªõp t·ª´ kh√≥a: {keyword_match}, D·ª± ƒëo√°n cu·ªëi: {final_pred}")

            # Ki·ªÉm tra regex cho th√¥ng tin c√° nh√¢n
            if re.match(email_regex, line, re.IGNORECASE):
                personal_info["email"].append(line)
                logger.debug(f"Kh·ªõp email: {line}")
                continue
            if re.match(phone_regex, line, re.IGNORECASE):
                phone_match = re.match(phone_regex, line, re.IGNORECASE)
                if phone_match:
                    phone_number = phone_match.group(0)
                    personal_info["phone"].append(phone_number)
                    logger.debug(f"Kh·ªõp s·ªë ƒëi·ªán tho·∫°i: {phone_number}")
                    remaining_text = line.replace(phone_number, "").strip()
                    if remaining_text and not any(keyword in normalize_text(remaining_text) for keyword in
                                                  KEYWORD_CATEGORIES["career_objective"] + KEYWORD_CATEGORIES[
                                                      "experience"]):
                        final_pred = "other"
                    else:
                        continue
                else:
                    continue
            if (re.match(name_regex, line, re.IGNORECASE) and
                    final_pred not in section_headers.values() and
                    not any(keyword in normalized_line for keyword in
                            KEYWORD_CATEGORIES["experience"] +
                            KEYWORD_CATEGORIES["skills_technology"] +
                            KEYWORD_CATEGORIES["skills_soft"] +
                            KEYWORD_CATEGORIES["skills_language"] +
                            sum((sub_keywords for sub_keywords in KEYWORD_CATEGORIES["skills_industry"].values()), []) +
                            KEYWORD_CATEGORIES["awards"] +
                            KEYWORD_CATEGORIES["activities"] +
                            KEYWORD_CATEGORIES["certificates"]) and
                    len(line.split()) >= 2 and len(line.split()) <= 4):
                if previous_line and len(previous_line.split()) < 2 and re.match(
                        r"[a-zA-Z\s√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]+", previous_line):
                    full_name = f"{previous_line} {line}".strip()
                    if re.match(name_regex, full_name, re.IGNORECASE):
                        personal_info["name"].append(full_name)
                        logger.debug(f"Kh·ªõp v√† g·ªôp t√™n: {full_name}")
                        previous_line = ""
                        continue
                personal_info["name"].append(line)
                logger.debug(f"Kh·ªõp t√™n: {line}")
                continue
            if re.match(address_regex, line, re.IGNORECASE) and not any(keyword in normalized_line for keyword in
                                                                        KEYWORD_CATEGORIES["experience"] +
                                                                        KEYWORD_CATEGORIES["career_objective"]):
                personal_info["address"].append(line)
                logger.debug(f"Kh·ªõp ƒë·ªãa ch·ªâ: {line}")
                continue
            if re.match(birth_date_regex, line):
                personal_info["birth_date"].append(line)
                logger.debug(f"Kh·ªõp ng√†y sinh: {line}")
                continue
            if re.match(gender_regex, line, re.IGNORECASE):
                personal_info["gender"].append(line)
                logger.debug(f"Kh·ªõp gi·ªõi t√≠nh: {line}")
                continue
            if any(keyword in line_lower for keyword in
                   KEYWORD_CATEGORIES["personal_info_name"]) and final_pred not in section_headers.values():
                personal_info["other"].append(line)
                logger.debug(f"Kh·ªõp th√¥ng tin c√° nh√¢n kh√°c: {line}")
                continue

            # X·ª≠ l√Ω c√°c danh m·ª•c theo final_pred
            if final_pred == "education":
                duration = re.search(duration_regex, line)
                year = re.search(year_regex, line)
                education.append({
                    "institution": line,
                    "duration": duration.group(0) if duration else "",
                    "year": year.group(0) if year else ""
                })

            elif final_pred in ("skills_technology", "skills_soft", "skills_language", "skills_industry"):
                # T√°ch k·ªπ nƒÉng linh ho·∫°t h∆°n
                skill_tokens = [
                    token.strip()
                    for token in re.split(
                        r',|;|:|-|/|\n|\s{2,}|(?<=[a-z])\s+(?=[A-Z])|(?<=[A-Z])\s+(?=[A-Z][a-z])',
                        line
                    )
                    if token.strip()
                ]
                for token in skill_tokens:
                    normalized_token = normalize_text(token)
                    # B·ªè qua n·∫øu k·ªπ nƒÉng qu√° d√†i (>= 6 t·ª´) -> kh·∫£ nƒÉng l√† m√¥ t·∫£
                    if len(token.split()) > 5:
                        continue

                    is_skill = False
                    skill_category = None
                    if final_pred == "skills_industry":
                        for sub_category, sub_keywords in KEYWORD_CATEGORIES["skills_industry"].items():
                            if any(keyword in normalized_token for keyword in sub_keywords):
                                is_skill = True
                                skill_category = sub_category
                                break
                    else:
                        if any(keyword in normalized_token for keyword in KEYWORD_CATEGORIES[final_pred]):
                            is_skill = True
                            skill_category = final_pred

                    if is_skill:
                        # Ki·ªÉm tra tr√πng tr∆∞·ªõc khi th√™m
                        if not any(s["name"].lower() == token.lower() for s in skills):
                            skills.append({
                                "name": token,
                                "category": skill_category
                            })
                            logger.debug(f"Kh·ªõp k·ªπ nƒÉng: {token}, Danh m·ª•c: {skill_category}")
            elif final_pred == "projects":
                projects.append({
                    "name": line,
                    "duration": re.search(duration_regex, line).group(0) if re.search(duration_regex, line) else ""
                })
            elif final_pred == "experience":
                duration = re.search(duration_regex, line)
                if re.match(company_regex, line, re.IGNORECASE) or any(
                        word in line_lower for word in KEYWORD_CATEGORIES["experience"]):
                    experience.append({
                        "company": line,
                        "duration": duration.group(0) if duration else ""
                    })
                else:
                    experience.append({
                        "description": line,
                        "duration": duration.group(0) if duration else ""
                    })
            elif final_pred == "hobbies":
                if any(word in line_lower for word in KEYWORD_CATEGORIES["hobbies"]):
                    hobbies.append(line)
            elif final_pred == "career_objective":
                career_objective.append(line)
            elif final_pred == "certificates":
                if any(word in line_lower for word in KEYWORD_CATEGORIES["certificates"]):
                    year = re.search(year_regex, line)
                    certificates.append({
                        "name": line,
                        "year": year.group(0) if year else ""
                    })
            elif final_pred == "awards":
                if any(word in line_lower for word in KEYWORD_CATEGORIES["awards"]):
                    year = re.search(year_regex, line)
                    awards.append({
                        "name": line,
                        "year": year.group(0) if year else ""
                    })
            elif final_pred == "activities":
                duration = re.search(duration_regex, line)
                activities.append({
                    "name": line,
                    "duration": duration.group(0) if duration else ""
                })
            elif final_pred == "references":
                reference_buffer.append(line)

            previous_line = line

        # G·ªôp references
        if reference_buffer:
            references.append({"name": " ".join(reference_buffer)})

        result = {
            "status": "success",
            "extracted": {
                "personal_info": personal_info,
                "education": education,
                "career_objective": career_objective,
                "skills": skills,
                "projects": projects,
                "experience": experience,
                "certificates": certificates,
                "awards": awards,
                "activities": activities,
            }
        }
        logger.info(f"Th√¥ng tin c√° nh√¢n ƒë√£ tr√≠ch xu·∫•t: {personal_info['name']}")
        logger.info(f"K·ªπ nƒÉng ƒë√£ tr√≠ch xu·∫•t: {[skill['name'] for skill in skills]}")
        logger.info("Ho√†n th√†nh tr√≠ch xu·∫•t CV")
        return result
    except Exception as e:
        logger.error(f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ CV: {str(e)}")
        return {
            "status": "failed",
            "error_message": f"L·ªói khi tr√≠ch xu·∫•t th√¥ng tin t·ª´ CV: {str(e)}"
        }